{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression & Overfitting Analysis\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We want to fit a polynomial curve to noisy data sampled from a sinusoidal function.\n",
    "The goal is to observe how **model complexity** (polynomial degree $M$) affects:\n",
    "- Training error\n",
    "- Test error (generalization)\n",
    "\n",
    "This demonstrates the classic **bias-variance tradeoff** and **overfitting** phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "The underlying true function is:\n",
    "$$f(x) = \\sin(2\\pi x)$$\n",
    "\n",
    "We generate noisy observations:\n",
    "$$t = \\sin(2\\pi x) + \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Parameters:\n",
    "- $\\sigma = 0.3$ (noise standard deviation)\n",
    "- $x \\sim \\mathcal{U}(0, 1)$ (uniform distribution)\n",
    "- Training set: $N_{train} = 10$ samples\n",
    "- Test set: $N_{test} = 100$ samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Model Definition\n",
    "\n",
    "We model the data using a polynomial of degree $M$:\n",
    "$$y(x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\cdots + w_M x^M = \\sum_{j=0}^{M} w_j x^j$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w} = [w_0, w_1, \\ldots, w_M]^T$ is the weight vector\n",
    "- $M$ is the polynomial degree (model complexity parameter)\n",
    "\n",
    "Key insight: Although the model is polynomial in $x$, it is **linear in the parameters** $\\mathbf{w}$, making this a linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Matrix Construction\n",
    "\n",
    "For $N$ data points $\\{x_1, x_2, \\ldots, x_N\\}$, we construct the **design matrix** $\\mathbf{\\Phi}$:\n",
    "\n",
    "$$\\mathbf{\\Phi} = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\cdots & x_1^M \\\\ 1 & x_2 & x_2^2 & \\cdots & x_2^M \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_N & x_N^2 & \\cdots & x_N^M \\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{\\Phi}$ is an $N \\times (M+1)$ matrix\n",
    "- Each row $i$: $\\phi(x_i)^T = [1, x_i, x_i^2, \\ldots, x_i^M]$\n",
    "- The $j$-th column contains the $j$-th power of all input values\n",
    "\n",
    "The predictions can then be written as:\n",
    "$$\\mathbf{y} = \\mathbf{\\Phi} \\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Solution\n",
    "\n",
    "### Cost Function\n",
    "We minimize the sum-of-squares error:\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{N} (y(x_i, \\mathbf{w}) - t_i)^2 = \\frac{1}{2} ||\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{t}||^2$$\n",
    "\n",
    "### Deriving the Optimal Solution\n",
    "\n",
    "1. Expand the cost function:\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{t})^T(\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{t})$$\n",
    "$$= \\frac{1}{2} (\\mathbf{w}^T\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} - 2\\mathbf{w}^T\\mathbf{\\Phi}^T\\mathbf{t} + \\mathbf{t}^T\\mathbf{t})$$\n",
    "\n",
    "2. Take the gradient with respect to $\\mathbf{w}$:\n",
    "$$\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} - \\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "3. Set gradient to zero and solve:\n",
    "$$\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "### Closed-Form Solution (Normal Equations)\n",
    "$$\\mathbf{w}^* = (\\mathbf{\\Phi}^T\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "*Note: This solution exists and is unique when $\\mathbf{\\Phi}^T\\mathbf{\\Phi}$ is invertible. For high-degree polynomials with few data points, numerical issues may arise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metric: Root Mean Square Error\n",
    "\n",
    "To evaluate model performance, we use the Root Mean Square (RMS) error:\n",
    "\n",
    "$$E_{RMS} = \\sqrt{\\frac{2 \\cdot J(\\mathbf{w})}{N}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y(x_i, \\mathbf{w}) - t_i)^2}$$\n",
    "\n",
    "where:\n",
    "- $J(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{N} (y_i - t_i)^2$ is our cost function\n",
    "- Dividing by $N$ allows comparison across datasets of different sizes\n",
    "- Taking the square root gives an error in the same units as the target\n",
    "\n",
    "We compute $E_{RMS}$ separately for:\n",
    "- **Training set**: Measures how well the model fits the training data\n",
    "- **Test set**: Measures how well the model generalizes to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs Test Evaluation\n",
    "\n",
    "For each polynomial degree $M \\in \\{0, 1, 2, \\ldots, 9\\}$:\n",
    "\n",
    "1. **Build** the design matrix $\\mathbf{\\Phi}_{train}$ using training inputs\n",
    "2. **Solve** for optimal weights: $\\mathbf{w}^* = (\\mathbf{\\Phi}_{train}^T\\mathbf{\\Phi}_{train})^{-1}\\mathbf{\\Phi}_{train}^T\\mathbf{t}_{train}$\n",
    "3. **Predict** on training set: $\\mathbf{y}_{train} = \\mathbf{\\Phi}_{train}\\mathbf{w}^*$\n",
    "4. **Predict** on test set: $\\mathbf{y}_{test} = \\mathbf{\\Phi}_{test}\\mathbf{w}^*$\n",
    "5. **Compute** $E_{RMS}^{train}$ and $E_{RMS}^{test}$\n",
    "\n",
    "### Procedure Summary\n",
    "\n",
    "| Step | Formula | Description |\n",
    "|------|---------|-------------|\n",
    "| Design Matrix | $\\Phi_{ij} = x_i^j$ | Powers of input features |\n",
    "| Solve Weights | $\\mathbf{w}^* = (\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf{t}$ | Closed-form solution |\n",
    "| Predict | $\\mathbf{y} = \\Phi\\mathbf{w}^*$ | Matrix-vector product |\n",
    "| Error | $E_{RMS} = \\sqrt{\\frac{1}{N}||\\mathbf{y} - \\mathbf{t}||^2}$ | RMS error |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results: Overfitting Behavior\n",
    "\n",
    "### Typical Error Curves\n",
    "\n",
    "As polynomial degree $M$ increases:\n",
    "\n",
    "| $M$ | Training Error | Test Error | Interpretation |\n",
    "|-----|----------------|------------|----------------|\n",
    "| Low (0-1) | High | High | **Underfitting**: Model too simple |\n",
    "| Medium (2-3) | Medium | Low | **Good fit**: Captures pattern |\n",
    "| High (7-9) | Very Low | Very High | **Overfitting**: Fits noise |\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Training error** monotonically decreases as $M$ increases\n",
    "   - More parameters = more flexibility to fit training data\n",
    "   - At $M = N - 1$, polynomial passes through all training points (zero training error)\n",
    "\n",
    "2. **Test error** follows a U-shaped curve:\n",
    "   - Initially decreases (better model fits underlying pattern)\n",
    "   - Eventually increases (model starts fitting noise)\n",
    "\n",
    "3. **The gap** between training and test error indicates overfitting\n",
    "   - Small gap: Good generalization\n",
    "   - Large gap: Overfitting\n",
    "\n",
    "### Why Overfitting Occurs\n",
    "\n",
    "With limited training data ($N = 10$) and high model complexity ($M = 9$):\n",
    "- The model has 10 parameters to fit 10 data points\n",
    "- It can perfectly interpolate the noisy training data\n",
    "- But the learned function oscillates wildly between data points\n",
    "- Poor generalization to new test points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Training Set Size\n",
    "\n",
    "When we increase training data to $N = 100$:\n",
    "\n",
    "1. **Overfitting is reduced** - more data constraints the model\n",
    "2. **Test error stays lower** for higher-degree polynomials\n",
    "3. **Gap closes** - training and test error become more similar\n",
    "\n",
    "This demonstrates that **more data is a natural regularizer** against overfitting.\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "To avoid overfitting: $N >> M + 1$ (number of samples much greater than number of parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
